{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64acc6-17f3-4071-800c-4f3c1f6e4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('asset.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab41801-4749-4df8-a977-e97ede12e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43a804-bac6-44e1-8e0a-29904435d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'] = df['brand'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1bad3-9226-4500-94d1-368735b8dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808a5ea-cc52-409c-9b24-aba5cd186e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfc56b-1801-40b1-a030-38b3de3f86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf8d61-2a3b-40ca-9632-ae5642abf6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['item'].isin(['SSD', 'VGA CARD'])]\n",
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4ff90-3735-47da-8403-f78a967e8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df[df['serial_number'].notna()].copy()\n",
    "print(f\"‚úÖ Valid serial numbers: {len(df_valid)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c91bf4-bde4-481a-8d06-3b6c43847d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA LOADING & CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\nüìÇ [1/6] Loading and Cleaning Data...\")\n",
    "print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "\n",
    "# Remove rows with missing critical data\n",
    "df_clean = df[df['serial_number'].notna() & df['item'].notna()].copy()\n",
    "print(f\"‚úÖ After removing NaN: {len(df_clean)} records\")\n",
    "\n",
    "# Handle duplicates\n",
    "duplicates = df_clean[df_clean.duplicated(subset=['serial_number'], keep=False)]\n",
    "print(f\"‚ö†Ô∏è  Found {len(duplicates)} duplicate serial numbers\")\n",
    "\n",
    "# Keep first occurrence of duplicates\n",
    "df_clean = df_clean.drop_duplicates(subset=['serial_number'], keep='first')\n",
    "print(f\"‚úÖ After deduplication: {len(df_clean)} records\")\n",
    "\n",
    "# Recalculate features after truncation\n",
    "def extract_features(sn):\n",
    "    sn = str(sn).strip()\n",
    "    return {\n",
    "        'length': len(sn),\n",
    "        'digit_count': sum(c.isdigit() for c in sn),\n",
    "        'letter_count': sum(c.isalpha() for c in sn),\n",
    "        'first_3_chars': sn[:3].upper() if len(sn) >= 3 else sn.upper(),\n",
    "        'first_char': sn[0].upper() if len(sn) > 0 else '',\n",
    "        'digit_ratio': sum(c.isdigit() for c in sn) / len(sn) if len(sn) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Re-extract features for affected rows\n",
    "features_updated = df_clean['serial_number'].apply(extract_features)\n",
    "features_df_updated = pd.DataFrame(features_updated.tolist())\n",
    "\n",
    "# Update only the feature columns\n",
    "feature_cols = ['length', 'digit_count', 'letter_count', 'first_3_chars', 'first_char', 'digit_ratio']\n",
    "df_clean[feature_cols] = features_df_updated[feature_cols]\n",
    "\n",
    "# Clean Docking Station anomalies (length > 40)\n",
    "docking_anomalies = df_clean[(df_clean['item'] == 'DOCKING') & (df_clean['length'] > 40)]\n",
    "print(f\"‚ö†Ô∏è  Found {len(docking_anomalies)} Docking Station anomalies (length > 40)\")\n",
    "\n",
    "# Truncate long serial numbers to first 20 characters\n",
    "df_clean.loc[df_clean['length'] > 40, 'serial_number'] = df_clean.loc[\n",
    "    df_clean['length'] > 40, 'serial_number'\n",
    "].str[:20]\n",
    "\n",
    "print(f\"‚úÖ Data cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28aa4e-d9a7-4b35-b4d4-8a7a2fe06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: RULE-BASED CLASSIFIER (Baseline)\n",
    "# ============================================================================\n",
    "print(\"\\nüßÆ [2/6] Building Rule-Based Classifier...\")\n",
    "\n",
    "def rule_based_predict(row):\n",
    "    \"\"\"Rule-based prediction based on prefix patterns\"\"\"\n",
    "    prefix = row['first_3_chars']\n",
    "    length = row['length']\n",
    "    \n",
    "    if not isinstance(prefix, str):\n",
    "        prefix = str(prefix) if pd.notna(prefix) else ''\n",
    "    \n",
    "    # Monitor patterns\n",
    "    if prefix in ['CN0', 'TH0', 'CNC', 'CNK', '6CM']:\n",
    "        return 'MONITOR'\n",
    "    \n",
    "    # Desktop patterns\n",
    "    if prefix in ['SGH', '1CZ', '4CE']:\n",
    "        return 'DESKTOP'\n",
    "    \n",
    "    # Laptop patterns\n",
    "    if prefix.startswith('5C') or prefix in ['CNU', 'DXN', 'HS7']:\n",
    "        return 'LAPTOP'\n",
    "    \n",
    "    # Headset patterns (2XX series)\n",
    "    if prefix.startswith('2') and length == 12:\n",
    "        if prefix in ['203', '210']:\n",
    "            return 'EARPHONE'\n",
    "        else:\n",
    "            return 'HEADSET'\n",
    "    \n",
    "    # Docking patterns\n",
    "    if 'brand' in row and row['brand'] == 'dell' and prefix == 'CN0':\n",
    "        # CN0 could be monitor or docking, check length\n",
    "        if length > 20:\n",
    "            return 'DOCKING'\n",
    "    \n",
    "    # Default: use length heuristic\n",
    "    if length <= 8:\n",
    "        return 'LAPTOP'\n",
    "    elif length <= 11:\n",
    "        return 'DESKTOP'\n",
    "    elif length == 12:\n",
    "        return 'HEADSET'\n",
    "    elif length > 20:\n",
    "        return 'DOCKING'\n",
    "    else:\n",
    "        return 'MONITOR'\n",
    "        \n",
    "df_clean['rule_based_prediction'] = df_clean.apply(rule_based_predict, axis=1)\n",
    "\n",
    "# Evaluate rule-based model\n",
    "rb_accuracy = accuracy_score(df_clean['item'], df_clean['rule_based_prediction'])\n",
    "print(f\"‚úÖ Rule-Based Accuracy: {rb_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b10a7-0c6b-45f1-9f5a-6277f98ac2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: FEATURE ENGINEERING FOR ML\n",
    "# ============================================================================\n",
    "print(\"\\nüîß [3/6] Feature Engineering for ML Model...\")\n",
    "\n",
    "# Encode categorical features\n",
    "le_prefix = LabelEncoder()\n",
    "le_first_char = LabelEncoder()\n",
    "\n",
    "df_clean['prefix_encoded'] = le_prefix.fit_transform(df_clean['first_3_chars'])\n",
    "df_clean['first_char_encoded'] = le_first_char.fit_transform(df_clean['first_char'])\n",
    "\n",
    "# Select features for ML model\n",
    "feature_columns = [\n",
    "    'length', 'digit_count', 'letter_count', 'digit_ratio',\n",
    "    'prefix_encoded', 'first_char_encoded'\n",
    "]\n",
    "\n",
    "# Add brand if available\n",
    "if 'brand' in df_clean.columns:\n",
    "    df_clean['brand_filled'] = df_clean['brand'].fillna('UNKNOWN')\n",
    "    le_brand = LabelEncoder()\n",
    "    df_clean['brand_encoded'] = le_brand.fit_transform(df_clean['brand_filled'])\n",
    "    feature_columns.append('brand_encoded')\n",
    "\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['item']\n",
    "\n",
    "print(f\"‚úÖ Features prepared: {feature_columns}\")\n",
    "print(f\"‚úÖ Dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dfbf2-00ff-4f09-8e15-df3a743e14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: TRAIN RANDOM FOREST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nüå≤ [4/6] Training Random Forest Classifier...\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training set: {len(X_train)} samples\")\n",
    "print(f\"‚úÖ Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n‚úÖ Random Forest Accuracy: {rf_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüéØ Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, \n",
    "                   index=rf_model.classes_, \n",
    "                   columns=rf_model.classes_))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n‚≠ê Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9eaab0-672f-4ad3-81bd-152c5b88f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: SAVE MODEL & ARTIFACTS\n",
    "# ============================================================================\n",
    "print(\"\\nüíæ [5/6] Saving Model and Artifacts...\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, 'asset_predictor_model.pkl')\n",
    "print(\"‚úÖ Model saved: asset_predictor_model.pkl\")\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(le_prefix, 'encoder_prefix.pkl')\n",
    "joblib.dump(le_first_char, 'encoder_first_char.pkl')\n",
    "if 'brand' in df_clean.columns:\n",
    "    joblib.dump(le_brand, 'encoder_brand.pkl')\n",
    "print(\"‚úÖ Encoders saved\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_columns, 'feature_columns.pkl')\n",
    "print(\"‚úÖ Feature config saved\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_clean.to_csv('asset_cleaned.csv', index=False)\n",
    "print(\"‚úÖ Cleaned dataset saved: asset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215c01c-fc01-4440-a9c6-c16a45bdcb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: CREATE PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\nüîÆ [6/6] Creating Prediction API...\")\n",
    "\n",
    "def predict_asset_type(serial_number, brand=None):\n",
    "    # Extract features\n",
    "    sn = str(serial_number).strip()\n",
    "    features = extract_features(sn)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_data = {\n",
    "        'length': features['length'],\n",
    "        'digit_count': features['digit_count'],\n",
    "        'letter_count': features['letter_count'],\n",
    "        'digit_ratio': features['digit_ratio'],\n",
    "        'prefix_encoded': le_prefix.transform([features['first_3_chars']])[0] \n",
    "                          if features['first_3_chars'] in le_prefix.classes_ \n",
    "                          else 0,\n",
    "        'first_char_encoded': le_first_char.transform([features['first_char']])[0]\n",
    "                              if features['first_char'] in le_first_char.classes_\n",
    "                              else 0\n",
    "    }\n",
    "    \n",
    "    if 'brand_encoded' in feature_columns and brand:\n",
    "        brand_clean = brand if brand in le_brand.classes_ else 'UNKNOWN'\n",
    "        input_data['brand_encoded'] = le_brand.transform([brand_clean])[0]\n",
    "    elif 'brand_encoded' in feature_columns:\n",
    "        input_data['brand_encoded'] = le_brand.transform(['UNKNOWN'])[0]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    X_input = pd.DataFrame([input_data])[feature_columns]\n",
    "    \n",
    "    # Predict\n",
    "    prediction = rf_model.predict(X_input)[0]\n",
    "    probabilities = rf_model.predict_proba(X_input)[0]\n",
    "    confidence = max(probabilities)\n",
    "    \n",
    "    # Get rule-based prediction for comparison\n",
    "    rule_pred = rule_based_predict(pd.Series({\n",
    "        'first_3_chars': features['first_3_chars'],\n",
    "        'length': features['length'],\n",
    "        'brand': brand\n",
    "    }))\n",
    "    \n",
    "    return {\n",
    "        'serial_number': serial_number,\n",
    "        'predicted_item': prediction,\n",
    "        'confidence': f\"{confidence:.2%}\",\n",
    "        'rule_based_prediction': rule_pred,\n",
    "        'prefix': features['first_3_chars'],\n",
    "        'probabilities': dict(zip(rf_model.classes_, probabilities))\n",
    "    }\n",
    "\n",
    "# Save prediction function\n",
    "joblib.dump(predict_asset_type, 'prediction_function.pkl')\n",
    "print(\"‚úÖ Prediction function saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ DEMO PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with real examples from dataset\n",
    "test_samples = [\n",
    "    ('CN0ABCD12345', 'dell'),\n",
    "    ('SGH1234567', None),\n",
    "    ('5CG7890ABC', None),\n",
    "    ('251123456789', 'logitech'),\n",
    "]\n",
    "\n",
    "for sn, brand in test_samples:\n",
    "    result = predict_asset_type(sn, brand)\n",
    "    print(f\"\\nüìå Serial Number: {result['serial_number']}\")\n",
    "    print(f\"   Predicted: {result['predicted_item']} (Confidence: {result['confidence']})\")\n",
    "    print(f\"   Rule-Based: {result['rule_based_prediction']}\")\n",
    "    print(f\"   Prefix: {result['prefix']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ SYSTEM READY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüì¶ Generated Files:\")\n",
    "print(\"   1. asset_predictor_model.pkl - Trained model\")\n",
    "print(\"   2. encoder_*.pkl - Feature encoders\")\n",
    "print(\"   3. feature_columns.pkl - Feature configuration\")\n",
    "print(\"   4. prediction_function.pkl - Prediction API\")\n",
    "print(\"   5. asset_cleaned.csv - Cleaned dataset\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   from prediction_api import predict_asset_type\")\n",
    "print(\"   result = predict_asset_type('CN0ABC123', 'dell')\")\n",
    "print(\"   print(result['predicted_item'])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (AI Asset GPU)",
   "language": "python",
   "name": "aiasset-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
