{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db64acc6-17f3-4071-800c-4f3c1f6e4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/assets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab41801-4749-4df8-a977-e97ede12e432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "DELL         644\n",
       "LOGITECH     550\n",
       "HP           526\n",
       "Logitech     104\n",
       "Samsung       79\n",
       "Lenovo        20\n",
       "NVIDIA        20\n",
       "ViewSonic      6\n",
       "Dell           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb1bad3-9226-4500-94d1-368735b8dce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "DELL         644\n",
       "LOGITECH     550\n",
       "HP           526\n",
       "Logitech     104\n",
       "Samsung       79\n",
       "Lenovo        20\n",
       "NVIDIA        20\n",
       "ViewSonic      6\n",
       "Dell           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4808a5ea-cc52-409c-9b24-aba5cd186e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item\n",
       "MONITOR        960\n",
       "USB HEADSET    553\n",
       "DOCKING        237\n",
       "DESKTOP        125\n",
       "LAPTOP         121\n",
       "EARPHONE       101\n",
       "SSD             79\n",
       "VGA CARD        20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cfc56b-1801-40b1-a030-38b3de3f86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3578 entries, 0 to 3577\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   pr_ref           1670 non-null   object\n",
      " 1   po_ref           2831 non-null   object\n",
      " 2   item             2196 non-null   object\n",
      " 3   brand            1950 non-null   object\n",
      " 4   type             3577 non-null   object\n",
      " 5   serial_number    3488 non-null   object\n",
      " 6   ghrs_id          2092 non-null   object\n",
      " 7   badge_id         1375 non-null   object\n",
      " 8   assignment_date  1122 non-null   object\n",
      " 9   location         2540 non-null   object\n",
      " 10  remarks          2039 non-null   object\n",
      " 11  dept/project     2344 non-null   object\n",
      " 12  delivery_date    1012 non-null   object\n",
      " 13  status           3578 non-null   object\n",
      " 14  service_tag      126 non-null    object\n",
      " 15  username         2202 non-null   object\n",
      " 16  device_name      1450 non-null   object\n",
      " 17  specifications   1379 non-null   object\n",
      "dtypes: object(18)\n",
      "memory usage: 503.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bf8d61-2a3b-40ca-9632-ae5642abf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item\n",
       "MONITOR        960\n",
       "USB HEADSET    553\n",
       "DOCKING        237\n",
       "DESKTOP        125\n",
       "LAPTOP         121\n",
       "EARPHONE       101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~df['item'].isin(['SSD', 'VGA CARD'])]\n",
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff4ff90-3735-47da-8403-f78a967e8072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Valid serial numbers: 3409 rows\n"
     ]
    }
   ],
   "source": [
    "df_valid = df[df['serial_number'].notna()].copy()\n",
    "print(f\"‚úÖ Valid serial numbers: {len(df_valid)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c91bf4-bde4-481a-8d06-3b6c43847d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ [1/6] Loading and Cleaning Data...\n",
      "‚úÖ Loaded 3479 records\n",
      "‚úÖ After removing NaN: 2029 records\n",
      "‚ö†Ô∏è  Found 0 duplicate serial numbers\n",
      "‚úÖ After deduplication: 2029 records\n",
      "‚ö†Ô∏è  Found 0 Docking Station anomalies (length > 40)\n",
      "‚úÖ Data cleaned successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA LOADING & CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\nüìÇ [1/6] Loading and Cleaning Data...\")\n",
    "print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "\n",
    "# Remove rows with missing critical data\n",
    "df_clean = df[df['serial_number'].notna() & df['item'].notna()].copy()\n",
    "print(f\"‚úÖ After removing NaN: {len(df_clean)} records\")\n",
    "\n",
    "# Handle duplicates\n",
    "duplicates = df_clean[df_clean.duplicated(subset=['serial_number'], keep=False)]\n",
    "print(f\"‚ö†Ô∏è  Found {len(duplicates)} duplicate serial numbers\")\n",
    "\n",
    "# Keep first occurrence of duplicates\n",
    "df_clean = df_clean.drop_duplicates(subset=['serial_number'], keep='first')\n",
    "print(f\"‚úÖ After deduplication: {len(df_clean)} records\")\n",
    "\n",
    "# Recalculate features after truncation\n",
    "def extract_features(sn):\n",
    "    sn = str(sn).strip()\n",
    "    return {\n",
    "        'length': len(sn),\n",
    "        'digit_count': sum(c.isdigit() for c in sn),\n",
    "        'letter_count': sum(c.isalpha() for c in sn),\n",
    "        'first_3_chars': sn[:3].upper() if len(sn) >= 3 else sn.upper(),\n",
    "        'first_char': sn[0].upper() if len(sn) > 0 else '',\n",
    "        'digit_ratio': sum(c.isdigit() for c in sn) / len(sn) if len(sn) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Re-extract features for affected rows\n",
    "features_updated = df_clean['serial_number'].apply(extract_features)\n",
    "features_df_updated = pd.DataFrame(features_updated.tolist())\n",
    "\n",
    "# Update only the feature columns\n",
    "feature_cols = ['length', 'digit_count', 'letter_count', 'first_3_chars', 'first_char', 'digit_ratio']\n",
    "df_clean[feature_cols] = features_df_updated[feature_cols]\n",
    "\n",
    "# Clean Docking Station anomalies (length > 40)\n",
    "docking_anomalies = df_clean[(df_clean['item'] == 'DOCKING') & (df_clean['length'] > 40)]\n",
    "print(f\"‚ö†Ô∏è  Found {len(docking_anomalies)} Docking Station anomalies (length > 40)\")\n",
    "\n",
    "# Truncate long serial numbers to first 20 characters\n",
    "df_clean.loc[df_clean['length'] > 40, 'serial_number'] = df_clean.loc[\n",
    "    df_clean['length'] > 40, 'serial_number'\n",
    "].str[:20]\n",
    "\n",
    "print(f\"‚úÖ Data cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff28aa4e-d9a7-4b35-b4d4-8a7a2fe06383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßÆ [2/6] Building Rule-Based Classifier...\n",
      "‚úÖ Rule-Based Accuracy: 48.69%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: RULE-BASED CLASSIFIER (Baseline)\n",
    "# ============================================================================\n",
    "print(\"\\nüßÆ [2/6] Building Rule-Based Classifier...\")\n",
    "\n",
    "def rule_based_predict(row):\n",
    "    \"\"\"Rule-based prediction based on prefix patterns\"\"\"\n",
    "    prefix = row['first_3_chars']\n",
    "    length = row['length']\n",
    "    \n",
    "    if not isinstance(prefix, str):\n",
    "        prefix = str(prefix) if pd.notna(prefix) else ''\n",
    "    \n",
    "    # Monitor patterns\n",
    "    if prefix in ['CN0', 'TH0', 'CNC', 'CNK', '6CM']:\n",
    "        return 'MONITOR'\n",
    "    \n",
    "    # Desktop patterns\n",
    "    if prefix in ['SGH', '1CZ', '4CE']:\n",
    "        return 'DESKTOP'\n",
    "    \n",
    "    # Laptop patterns\n",
    "    if prefix.startswith('5C') or prefix in ['CNU', 'DXN', 'HS7']:\n",
    "        return 'LAPTOP'\n",
    "    \n",
    "    # Headset patterns (2XX series)\n",
    "    if prefix.startswith('2') and length == 12:\n",
    "        if prefix in ['203', '210']:\n",
    "            return 'EARPHONE'\n",
    "        else:\n",
    "            return 'HEADSET'\n",
    "    \n",
    "    # Docking patterns\n",
    "    if 'brand' in row and row['brand'] == 'dell' and prefix == 'CN0':\n",
    "        # CN0 could be monitor or docking, check length\n",
    "        if length > 20:\n",
    "            return 'DOCKING'\n",
    "    \n",
    "    # Default: use length heuristic\n",
    "    if length <= 8:\n",
    "        return 'LAPTOP'\n",
    "    elif length <= 11:\n",
    "        return 'DESKTOP'\n",
    "    elif length == 12:\n",
    "        return 'HEADSET'\n",
    "    elif length > 20:\n",
    "        return 'DOCKING'\n",
    "    else:\n",
    "        return 'MONITOR'\n",
    "        \n",
    "df_clean['rule_based_prediction'] = df_clean.apply(rule_based_predict, axis=1)\n",
    "\n",
    "# Evaluate rule-based model\n",
    "rb_accuracy = accuracy_score(df_clean['item'], df_clean['rule_based_prediction'])\n",
    "print(f\"‚úÖ Rule-Based Accuracy: {rb_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722b10a7-0c6b-45f1-9f5a-6277f98ac2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß [3/6] Feature Engineering for ML Model...\n",
      "‚úÖ Features prepared: ['length', 'digit_count', 'letter_count', 'digit_ratio', 'prefix_encoded', 'first_char_encoded', 'brand_encoded']\n",
      "‚úÖ Dataset shape: (2029, 7)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: FEATURE ENGINEERING FOR ML\n",
    "# ============================================================================\n",
    "print(\"\\nüîß [3/6] Feature Engineering for ML Model...\")\n",
    "\n",
    "# Encode categorical features\n",
    "le_prefix = LabelEncoder()\n",
    "le_first_char = LabelEncoder()\n",
    "\n",
    "df_clean['prefix_encoded'] = le_prefix.fit_transform(df_clean['first_3_chars'])\n",
    "df_clean['first_char_encoded'] = le_first_char.fit_transform(df_clean['first_char'])\n",
    "\n",
    "# Select features for ML model\n",
    "feature_columns = [\n",
    "    'length', 'digit_count', 'letter_count', 'digit_ratio',\n",
    "    'prefix_encoded', 'first_char_encoded'\n",
    "]\n",
    "\n",
    "# Add brand if available\n",
    "if 'brand' in df_clean.columns:\n",
    "    df_clean['brand_filled'] = df_clean['brand'].fillna('UNKNOWN')\n",
    "    le_brand = LabelEncoder()\n",
    "    df_clean['brand_encoded'] = le_brand.fit_transform(df_clean['brand_filled'])\n",
    "    feature_columns.append('brand_encoded')\n",
    "\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['item']\n",
    "\n",
    "print(f\"‚úÖ Features prepared: {feature_columns}\")\n",
    "print(f\"‚úÖ Dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4dfbf2-00ff-4f09-8e15-df3a743e14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ [4/6] Training Random Forest Classifier...\n",
      "‚úÖ Training set: 1623 samples\n",
      "‚úÖ Test set: 406 samples\n",
      "\n",
      "‚úÖ Random Forest Accuracy: 99.01%\n",
      "\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     DESKTOP       0.96      0.92      0.94        25\n",
      "     DOCKING       1.00      1.00      1.00        48\n",
      "    EARPHONE       0.86      0.95      0.90        20\n",
      "      LAPTOP       1.00      1.00      1.00        24\n",
      "     MONITOR       1.00      1.00      1.00       192\n",
      " USB HEADSET       1.00      0.99      0.99        97\n",
      "\n",
      "    accuracy                           0.99       406\n",
      "   macro avg       0.97      0.98      0.97       406\n",
      "weighted avg       0.99      0.99      0.99       406\n",
      "\n",
      "\n",
      "üéØ Confusion Matrix:\n",
      "             DESKTOP  DOCKING  EARPHONE  LAPTOP  MONITOR  USB HEADSET\n",
      "DESKTOP           23        0         2       0        0            0\n",
      "DOCKING            0       48         0       0        0            0\n",
      "EARPHONE           1        0        19       0        0            0\n",
      "LAPTOP             0        0         0      24        0            0\n",
      "MONITOR            0        0         0       0      192            0\n",
      "USB HEADSET        0        0         1       0        0           96\n",
      "\n",
      "‚≠ê Feature Importance:\n",
      "              feature  importance\n",
      "6       brand_encoded    0.405043\n",
      "0              length    0.137509\n",
      "5  first_char_encoded    0.115638\n",
      "2        letter_count    0.100002\n",
      "1         digit_count    0.099716\n",
      "4      prefix_encoded    0.096635\n",
      "3         digit_ratio    0.045457\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: TRAIN RANDOM FOREST MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nüå≤ [4/6] Training Random Forest Classifier...\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training set: {len(X_train)} samples\")\n",
    "print(f\"‚úÖ Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n‚úÖ Random Forest Accuracy: {rf_accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüéØ Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(pd.DataFrame(cm, \n",
    "                   index=rf_model.classes_, \n",
    "                   columns=rf_model.classes_))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n‚≠ê Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9eaab0-672f-4ad3-81bd-152c5b88f7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ [5/6] Saving Model and Artifacts...\n",
      "‚úÖ Model saved: asset_predictor_model.pkl\n",
      "‚úÖ Encoders saved\n",
      "‚úÖ Feature config saved\n",
      "‚úÖ Cleaned dataset saved: asset_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: SAVE MODEL & ARTIFACTS\n",
    "# ============================================================================\n",
    "print(\"\\nüíæ [5/6] Saving Model and Artifacts...\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, 'asset_predictor_model.pkl')\n",
    "print(\"‚úÖ Model saved: asset_predictor_model.pkl\")\n",
    "\n",
    "# Save encoders\n",
    "joblib.dump(le_prefix, 'encoder_prefix.pkl')\n",
    "joblib.dump(le_first_char, 'encoder_first_char.pkl')\n",
    "if 'brand' in df_clean.columns:\n",
    "    joblib.dump(le_brand, 'encoder_brand.pkl')\n",
    "print(\"‚úÖ Encoders saved\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_columns, 'feature_columns.pkl')\n",
    "print(\"‚úÖ Feature config saved\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_clean.to_csv('asset_cleaned.csv', index=False)\n",
    "print(\"‚úÖ Cleaned dataset saved: asset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e215c01c-fc01-4440-a9c6-c16a45bdcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ [6/6] Creating Prediction API...\n",
      "‚úÖ Prediction function saved\n",
      "\n",
      "======================================================================\n",
      "üéØ DEMO PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "üìå Serial Number: CN0ABCD12345\n",
      "   Predicted: DESKTOP (Confidence: 50.91%)\n",
      "   Rule-Based: MONITOR\n",
      "   Prefix: CN0\n",
      "\n",
      "üìå Serial Number: SGH1234567\n",
      "   Predicted: DESKTOP (Confidence: 100.00%)\n",
      "   Rule-Based: DESKTOP\n",
      "   Prefix: SGH\n",
      "\n",
      "üìå Serial Number: 5CG7890ABC\n",
      "   Predicted: DESKTOP (Confidence: 66.04%)\n",
      "   Rule-Based: LAPTOP\n",
      "   Prefix: 5CG\n",
      "\n",
      "üìå Serial Number: 251123456789\n",
      "   Predicted: DESKTOP (Confidence: 95.99%)\n",
      "   Rule-Based: HEADSET\n",
      "   Prefix: 251\n",
      "\n",
      "======================================================================\n",
      "‚úÖ SYSTEM READY!\n",
      "======================================================================\n",
      "\n",
      "üì¶ Generated Files:\n",
      "   1. asset_predictor_model.pkl - Trained model\n",
      "   2. encoder_*.pkl - Feature encoders\n",
      "   3. feature_columns.pkl - Feature configuration\n",
      "   4. prediction_function.pkl - Prediction API\n",
      "   5. asset_cleaned.csv - Cleaned dataset\n",
      "\n",
      "üí° Usage:\n",
      "   from prediction_api import predict_asset_type\n",
      "   result = predict_asset_type('CN0ABC123', 'dell')\n",
      "   print(result['predicted_item'])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: CREATE PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\nüîÆ [6/6] Creating Prediction API...\")\n",
    "\n",
    "def predict_asset_type(serial_number, brand=None):\n",
    "    # Extract features\n",
    "    sn = str(serial_number).strip()\n",
    "    features = extract_features(sn)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_data = {\n",
    "        'length': features['length'],\n",
    "        'digit_count': features['digit_count'],\n",
    "        'letter_count': features['letter_count'],\n",
    "        'digit_ratio': features['digit_ratio'],\n",
    "        'prefix_encoded': le_prefix.transform([features['first_3_chars']])[0] \n",
    "                          if features['first_3_chars'] in le_prefix.classes_ \n",
    "                          else 0,\n",
    "        'first_char_encoded': le_first_char.transform([features['first_char']])[0]\n",
    "                              if features['first_char'] in le_first_char.classes_\n",
    "                              else 0\n",
    "    }\n",
    "    \n",
    "    if 'brand_encoded' in feature_columns and brand:\n",
    "        brand_clean = brand if brand in le_brand.classes_ else 'UNKNOWN'\n",
    "        input_data['brand_encoded'] = le_brand.transform([brand_clean])[0]\n",
    "    elif 'brand_encoded' in feature_columns:\n",
    "        input_data['brand_encoded'] = le_brand.transform(['UNKNOWN'])[0]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    X_input = pd.DataFrame([input_data])[feature_columns]\n",
    "    \n",
    "    # Predict\n",
    "    prediction = rf_model.predict(X_input)[0]\n",
    "    probabilities = rf_model.predict_proba(X_input)[0]\n",
    "    confidence = max(probabilities)\n",
    "    \n",
    "    # Get rule-based prediction for comparison\n",
    "    rule_pred = rule_based_predict(pd.Series({\n",
    "        'first_3_chars': features['first_3_chars'],\n",
    "        'length': features['length'],\n",
    "        'brand': brand\n",
    "    }))\n",
    "    \n",
    "    return {\n",
    "        'serial_number': serial_number,\n",
    "        'predicted_item': prediction,\n",
    "        'confidence': f\"{confidence:.2%}\",\n",
    "        'rule_based_prediction': rule_pred,\n",
    "        'prefix': features['first_3_chars'],\n",
    "        'probabilities': dict(zip(rf_model.classes_, probabilities))\n",
    "    }\n",
    "\n",
    "# Save prediction function\n",
    "joblib.dump(predict_asset_type, 'prediction_function.pkl')\n",
    "print(\"‚úÖ Prediction function saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ DEMO PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with real examples from dataset\n",
    "test_samples = [\n",
    "    ('CN0ABCD12345', 'dell'),\n",
    "    ('SGH1234567', None),\n",
    "    ('5CG7890ABC', None),\n",
    "    ('251123456789', 'logitech'),\n",
    "]\n",
    "\n",
    "for sn, brand in test_samples:\n",
    "    result = predict_asset_type(sn, brand)\n",
    "    print(f\"\\nüìå Serial Number: {result['serial_number']}\")\n",
    "    print(f\"   Predicted: {result['predicted_item']} (Confidence: {result['confidence']})\")\n",
    "    print(f\"   Rule-Based: {result['rule_based_prediction']}\")\n",
    "    print(f\"   Prefix: {result['prefix']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ SYSTEM READY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüì¶ Generated Files:\")\n",
    "print(\"   1. asset_predictor_model.pkl - Trained model\")\n",
    "print(\"   2. encoder_*.pkl - Feature encoders\")\n",
    "print(\"   3. feature_columns.pkl - Feature configuration\")\n",
    "print(\"   4. prediction_function.pkl - Prediction API\")\n",
    "print(\"   5. asset_cleaned.csv - Cleaned dataset\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   from prediction_api import predict_asset_type\")\n",
    "print(\"   result = predict_asset_type('CN0ABC123', 'dell')\")\n",
    "print(\"   print(result['predicted_item'])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (AI Asset GPU)",
   "language": "python",
   "name": "aiasset-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
